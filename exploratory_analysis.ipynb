{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import math\n",
    "from datetime import datetime\n",
    "import re\n",
    "import csv\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.stats.api as sms\n",
    "import scipy\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import tensorflow as tf\n",
    "from pipeline import data, features, models, consts\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import shap\n",
    "import pickle\n",
    "import xgboost\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "#datafile = Path(\"/mnt/c/Users/anbag/Box Sync/Research/UVA/Medication Adherance/MEMS dataset/final_merged_set_v6.csv\")\n",
    "datafile = Path(\"/mnt/c/Users/ab5bt/Box Sync/Research/UVA/Medication Adherance/MEMS dataset/final_merged_set_v6.csv\")\n",
    "df = pd.read_csv(datafile, parse_dates=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning & Feature Engineering\n",
    "Thank you to Jason Brownlee\n",
    "https://machinelearningmastery.com/basic-data-cleaning-for-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Dataset class\n",
    "dataset = data.Dataset(df, id_col = 'PtID')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Perform an initial cleaning of the dataset ----------\n",
    "dataset.clean(to_rename = {**consts.RENAMINGS['demographics'], \n",
    "                             **consts.RENAMINGS['medical']}, \n",
    "              to_drop=[col for col in dataset.df.columns if '_Name' in col] + \n",
    "                      ['MemsNum', 'Monitor'],\n",
    "              to_map = consts.CODEBOOK,\n",
    "              to_binarize = ['race_other'],\n",
    "              onehots_to_reverse = ['race_']\n",
    "             )\n",
    "\n",
    "''' Set dtypes on remaining columns\n",
    "For now, naively assume we only have numerics, datetimes, or objects\n",
    "'''\n",
    "dtypes_dict = {\n",
    "    'numeric': [col for col in dataset.df.columns if 'date' not in col.lower()],\n",
    "    'datetime': ['DateEnroll'],\n",
    "    'categorical': list(consts.CODEBOOK.keys()) + ['race']\n",
    "}\n",
    "\n",
    "dataset.set_dtypes(dtypes_dict)\n",
    "dataset.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.df.education.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dataset.df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate static (non-temporal) features from measures such as validate instruments (e.g., FACTB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize candidate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    " Organize the candidate features into useful categories for later reference\n",
    " A bit tedious, but helpful \n",
    "'''\n",
    "\n",
    "# Set our excluded features, before anything else\n",
    "excluded = ['percentMEMS8']  # Overall adherence rate - unlikely to be used since we're building weekly vectors\n",
    "\n",
    "dataset.update_feature_categories({\n",
    "    'demographics': [v for v in consts.RENAMINGS['demographics'].values()\n",
    "                     if v in dataset.df.columns] + ['race'], #add the new, single race col\n",
    "    'study_behavior': [col for col in ['DateEnroll', 'Group', 'complete_4', \n",
    "                                       'complete_8', 'memsuse', 'deceased',\n",
    "                                       'day_miss_fromB', 'day_miss_from7', 'total_days_8'] \n",
    "                       if col in dataset.df.columns],\n",
    "    'medical': [v for v in consts.RENAMINGS['medical'].values() if v in dataset.df.columns] + \\\n",
    "               [col for col in ['early_late', 'diagtoenroll'] \n",
    "                if col in dataset.df.columns]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This dataset has several repeated measures for validated instruments, \n",
    "such as the FACTB\n",
    "\n",
    "Columns for repeated measures for the same instrument share a suffix (e.g., '_FACTB')\n",
    "Use regex to populate the `scores` category subdictionary quickly, using these suffixes\n",
    "''' \n",
    "\n",
    "# TODO: Fix scores so that we only have one column per score\n",
    "# incorporate the shift halfway through the study (i.e. midpoint assessments)\n",
    "\n",
    "for k,v in consts.SCORES.items():\n",
    "    ''' Handle special case of BCPT before doing anything else '''\n",
    "    if k == 'BCPT':\n",
    "        dataset.df.drop(\n",
    "            list(dataset.df.filter(regex = '_BCPT\\d*YN$')), \n",
    "            axis = 1, \n",
    "            inplace = True\n",
    "        )\n",
    "        dataset.df.drop(\n",
    "            list(dataset.df.filter(regex = '_BCPT\\d*O$')), \n",
    "            axis = 1, \n",
    "            inplace = True\n",
    "        )\n",
    "    \n",
    "    '''Some measures weren't precalculated. Let's fix this \n",
    "    We'll only focus on time point A, since it doesn't make sense to make predictions using\n",
    "    future scores!\n",
    "    ''' \n",
    "    if v['precalculated'] == False:\n",
    "        \n",
    "        ''' For the baseline time point, get the aggregate score and add it to the dataset\n",
    "        as a new column'''\n",
    "        prefix = 'A'\n",
    "        score_cols = list(\n",
    "            dataset.df.filter(regex='^' + prefix + v['suffix'] + '\\d*').columns\n",
    "        )\n",
    "        \n",
    "        dataset.df[prefix + v['suffix']] = dataset.df[score_cols].sum(axis=1)\n",
    "    '''We'll include this new column as a feature shortly'''\n",
    "   \n",
    "    dataset.update_feature_categories({\n",
    "        'scores': [prefix + v['suffix']]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create a catch-all category of remaining features, to ensure we got everything '''\n",
    "dataset.update_feature_categories({\n",
    "    'other': [col for col in dataset.df.columns \n",
    "              if col not in list(itertools.chain(*dataset.feature_categories.values())) # exclude anything already in the list\n",
    "              and not any(prefix in col for prefix in ['A_', 'B_', 'C_']) # exclude individual score cols\n",
    "              and 'date' not in col \n",
    "              and col not in dataset.id_col\n",
    "              and col not in excluded\n",
    "             ]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create new columns for several demographic and medical variables\n",
    "Be sure we update the feature categories dictionary '''\n",
    "\n",
    "demog_drug_cols = [col for col in dataset.df.columns if 'A_DEMO13DRUG' in col]\n",
    "newcol = 'DEMOG_numdrugs'\n",
    "dataset.df[newcol] = dataset.df[demog_drug_cols].count(axis=1)\n",
    "dataset.update_feature_categories({'demographics': [newcol]})\n",
    "\n",
    "post_exam_cols = [col for col in dataset.df.columns if 'C_MR5_date' in col]\n",
    "dataset.df[post_exam_cols] = dataset.df[post_exam_cols].apply(\n",
    "    lambda x: pd.to_datetime(x, errors='coerce')\n",
    ")\n",
    "newcol = 'C_numexams'\n",
    "dataset.df[newcol] = dataset.df[post_exam_cols].count(axis=1)\n",
    "dataset.update_feature_categories({'medical': [newcol]})\n",
    "\n",
    "''' Thank you @benvc!\n",
    "https://stackoverflow.com/questions/54367491/calculate-average-of-days-between-a-list-of-dates\n",
    "'''\n",
    "\n",
    "# TODO: Dates aren't necessarily in order. Ask Kristi if this is a data entry issue or \n",
    "# An ordering issue?\n",
    "newcol = 'mean_days_betw_exams'\n",
    "dataset.df[newcol] = dataset.df[post_exam_cols].apply(\n",
    "    lambda x: features.mean_days_between_dates(x),\n",
    "    axis=1\n",
    ")\n",
    "dataset.update_feature_categories({'medical': [newcol]})\n",
    "\n",
    "# Ensure everything looks good\n",
    "print(dataset.df['DEMOG_numdrugs'].head())\n",
    "print(dataset.df['C_numexams'].head())\n",
    "print(dataset.df['mean_days_betw_exams'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Impute numerics and categoricals (encoded as nominal numerics) ----\n",
    "numeric_cols = set(list(dataset.df.select_dtypes('number').columns))-set([dataset.id_col])\n",
    "\n",
    "# Let's be efficient and only impute our candidate features\n",
    "feature_cols = set(list(itertools.chain(\n",
    "                     *[v for k,v in dataset.feature_categories.items()])\n",
    "                   )\n",
    "                  )\n",
    "numeric_cols = list(numeric_cols.intersection(feature_cols))\n",
    "\n",
    "# Grab our categoricals as well\n",
    "categorical_cols = list(consts.CODEBOOK.keys()) + ['race']\n",
    "\n",
    "# Pre-imputation visual inspection\n",
    "dataset.df[categorical_cols].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the imputation\n",
    "dataset.df = features.impute(df=dataset.df, id_col=dataset.id_col, \n",
    "                             numerics=numeric_cols, categoricals=categorical_cols)\n",
    "\n",
    "# Imputation sanity check\n",
    "dataset.df[feature_cols].isnull().values.any()\n",
    "\n",
    "# Post-imputation visual inspection\n",
    "dataset.df[categorical_cols].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual inspection for multicollinearity\n",
    "sns.set(rc={'figure.figsize':(20, 20)})\n",
    "sns.heatmap(dataset.df[dataset.feature_categories['scores']].corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Remove highly-correlated scores as well as the FACT-G breast cancer subscale score \n",
    "(too correlated with other subscales - already marked that we shouldn't include it, \n",
    "in the consts.SCORES dictionary)'''\n",
    "\n",
    "new_scores = set(dataset.feature_categories['scores']) -\\\n",
    "                set(['A_BACS', 'A_MASES', 'A_BCPT', 'A_BC', 'A_CASE', 'A_FACITSP', 'A_MDASI'])\n",
    "dataset.update_feature_categories({'scores': list(new_scores)}, replace=True)\n",
    "\n",
    "# Re-inspect\n",
    "sns.set(rc={'figure.figsize':(20, 20)})\n",
    "sns.heatmap(dataset.df[dataset.feature_categories['scores']].corr(), annot=True)\n",
    "plt.show()\n",
    "\n",
    "''' Looks good, with one caveat: \n",
    "    We know the FACT-B, FACT-G, and FACT-G subscale scores are highly correlated (obvious)\n",
    "    We'll generate featuresets for each of these score types - one that uses only subscale scores, \n",
    "    one that uses only the full FACT-G score, and one that uses the full FACT-B score\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create featuresets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_featuresets = list()\n",
    "categories = list(dataset.feature_categories.keys())\n",
    "n_categories = len(categories)\n",
    "fact_subscales = ['A' + v['suffix'] for k,v in consts.SCORES.items() if v['subscale_include'] == True]\n",
    "drop_pairs = [\n",
    "    ('FACTB Subset', ['A_FACTG'] + fact_subscales), # FACT-B only\n",
    "    ('FACTB Subscales Subset', ['A_FACTB', 'A_FACTG']), # FACT-B subscales only\n",
    "    ('FACTG Subset', ['A_FACTB'] + fact_subscales) # FACT-G only\n",
    "]\n",
    "\n",
    "print(fact_subscales)\n",
    "\n",
    "import itertools\n",
    "for i in range(1, n_categories + 1):\n",
    "    for t in list(itertools.combinations(categories,i)):\n",
    "        subcategories = list(t)\n",
    "        name = ' + '.join(subcategories)\n",
    "        \n",
    "        '''For each featureset, create three further subsets related to FACT scores '''\n",
    "        df = dataset.build_df_from_feature_categories(subcategories)\n",
    "        \n",
    "        if 'scores' in subcategories:\n",
    "            \n",
    "            # If this is a featureset that includes scores, add the FACT-related subsets\n",
    "            for (subset_name, drop_cols) in drop_pairs:\n",
    "                df2 = df.drop(columns=drop_cols) # Returns a copy\n",
    "                static_featuresets.append(features.Featureset(df=df2, name=name + ' - ' + subset_name, \n",
    "                                                              id_col = dataset.id_col))\n",
    "        else:\n",
    "            static_featuresets.append(features.Featureset(df=df, name=name, id_col = dataset.id_col))\n",
    "        \n",
    "static_featuresets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic (Temporal) Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract temporal features by converting main dataset's df from wide-form to long-form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "# Get a list of all date columns\n",
    "date_cols = list(dataset.df.filter(regex='date\\d{3}$').columns)\n",
    "\n",
    "i = 0\n",
    "for col in date_cols:\n",
    "\n",
    "    # Find all the time cols for that date col\n",
    "    time_cols = list(dataset.df.filter(\n",
    "        regex='MEMS_{date_col}_time\\d{{1}}$'.format(date_col=col)).columns)  \n",
    "\n",
    "    ''' Perform a melt so we get MEMS events stratified by patient\n",
    "        Be sure to include the \"within range\" column as one of the id_vars''' \n",
    "    additional_cols = [\n",
    "        {\n",
    "            'original': 'MEMS_' + col + '_numtimes',\n",
    "            'new': 'num_times_used_today'\n",
    "        }\n",
    "    ]\n",
    "    if i > 0: # The first date won't have an interval or withinrange\n",
    "        additional_cols.append(\n",
    "            {\n",
    "                'original': 'MEMS_' + col + '_interval',\n",
    "                'new': 'interval'\n",
    "            }\n",
    "        )\n",
    "        additional_cols.append(\n",
    "            {\n",
    "                'original': 'MEMS_' + col + '_withinrange',\n",
    "                'new': 'withinrange'\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    all_id_col = [dataset.id_col, 'DateEnroll', col] + [x['original'] for x in additional_cols]\n",
    "    \n",
    "    res = dataset.df[all_id_col + time_cols].melt(id_vars = all_id_col)\n",
    "    \n",
    "    # Tidy up the resulting dataframe\n",
    "    res.rename(columns={col: 'date', 'value': 'time', 'variable': 'MEMS_day'}, \n",
    "               inplace=True)\n",
    "\n",
    "    res['MEMS_day'] =  res['MEMS_day'].apply(lambda x: int(re.sub(r'_time\\d*$', '', x.split('MEMS_date')[1])))\n",
    "    \n",
    "    res.rename(columns={x['original']:x['new'] for x in additional_cols},\n",
    "               inplace=True)\n",
    "\n",
    "#     res.drop(columns=['variable'], inplace=True)\n",
    "    \n",
    "    rows.append(res) # TODO - double check this...getting a weird warning about index alignment\n",
    "    i += 1\n",
    "\n",
    "df = pd.concat(rows, axis=0)\n",
    "\n",
    "# Create combined datetime column\n",
    "df['datetime'] = df.apply(\n",
    "    lambda x: features.get_datetime_col(x), axis=1\n",
    ")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "\n",
    "# Fix dtypes\n",
    "df[['withinrange', 'num_times_used_today']] = df[['withinrange', 'num_times_used_today']].fillna(0).astype(int)\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "df['interval'] = pd.to_timedelta(df['interval']) # Handle NaT intervals for first day?\n",
    "\n",
    "'''Drop rows with an empty date column.\n",
    "  Do NOT drop empty time columns - may have dates where it is recorded that the patient\n",
    "  did not use the cap. So, would have a date but no time. Need this info to calculate\n",
    "  additional stats later\n",
    "''' \n",
    "df.dropna(subset=['date'], inplace=True)\n",
    "\n",
    "# Remove observations that occurred before a subject's enrollment date\n",
    "df = df.loc[df['DateEnroll'] < df['date']]\n",
    "\n",
    "# Restrict to 210 MEMS days (not necessarily study days), per Kristi's documentation\n",
    "df = df[df['MEMS_day'] <= 210] \n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['PtID', 'date']).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check - Validate that we calculated days of adherence correctly\n",
    "df2 = df.groupby([dataset.id_col, 'MEMS_day'])['withinrange'].max().reset_index()\n",
    "df2 = df2.groupby(dataset.id_col)['withinrange'].sum().reset_index()\n",
    "df2 = df2.merge(dataset.df[[dataset.id_col, 'total_days_8']])\n",
    "df2.head(10)\n",
    "# Check!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add binary indicator of any usage (not just number of times used) on a given day\n",
    "df['used_today'] = df['num_times_used_today'].apply(\n",
    "    lambda x: 1 if x > 0 else 0\n",
    ")\n",
    "\n",
    "'''Generate epochs of interest (time of day, weekday, day/month of study, etc)\n",
    "   'time_of_day' category gets automatically encoded as a Categorical\n",
    "''' \n",
    "time_of_day_props = {\n",
    "    'bins': [-1, 6, 12, 18, 24],\n",
    "    'labels': ['early_morning', 'morning', 'afternoon', 'evening']\n",
    "}\n",
    "df = features.get_epochs(df, 'DateEnroll', 'PtID',\n",
    "                         time_of_day_props['bins'], \n",
    "                         time_of_day_props['labels'])\n",
    "df.head()\n",
    "\n",
    "# TODO - figure out a way to use the times of day? Not currently being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude first month (ramp-up period during which time users were getting used to the MEMS caps)\n",
    "df = df[df['study_month'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_featuresets = list()\n",
    "'''Group by our desired epoch and add standard metrics such as mean, std\n",
    "'''\n",
    "\n",
    "# TODO - add ability to calculate adherence metrics by month, extend epochs to include months\n",
    "# Will need a way to find days in a given month...\n",
    "\n",
    "for epoch in ['study_week']:\n",
    "    groupby_cols = [dataset.id_col, epoch]\n",
    "    temporal_feats = features.calc_standard_temporal_metrics(df, groupby_cols, 'datetime')\n",
    "    \n",
    "    # Calculate avg and standard deviation of number of times used\n",
    "    df2 = df.groupby(groupby_cols + ['study_day'])['num_times_used_today'].max().reset_index()\n",
    "    df2 = df.groupby(groupby_cols)['num_times_used_today'].agg({\n",
    "        'num_daily_events_mean': lambda x: x.sum() / consts.DAYS_IN_WEEK\n",
    "    }).reset_index()\n",
    "    temporal_feats = temporal_feats.merge(df2, on=groupby_cols)\n",
    "    \n",
    "    # Get most common time of day of event occurence\n",
    "    df2 = df.groupby(groupby_cols)['time_of_day'].agg({\n",
    "        'event_time_of_day_mode': pd.Series.mode\n",
    "    }).reset_index().drop(columns=['level_2'])\n",
    "    temporal_feats = temporal_feats.merge(df2, on=groupby_cols)\n",
    "    print(temporal_feats)\n",
    "    \n",
    "    # Calculate adherence rate (percentage of time in range)\n",
    "    df2 = df.groupby(groupby_cols + ['study_day'])['withinrange'].max().reset_index() # Max will be 1 or 0\n",
    "    df2 = df2.groupby(groupby_cols)['withinrange'].agg({\n",
    "        'adherence_rate': lambda x: x.sum() / consts.DAYS_IN_WEEK\n",
    "    }).reset_index()\n",
    "    temporal_feats = temporal_feats.merge(df2, on=groupby_cols)\n",
    "    \n",
    "    \n",
    "    # Impute and add to temporal featuresets dictionary\n",
    "    numeric_cols = list(set(list(temporal_feats.select_dtypes('number').columns)) -\\\n",
    "                set([dataset.id_col]))\n",
    "    \n",
    "    temporal_feats = features.common.impute(temporal_feats, dataset.id_col, numerics=numeric_cols)\n",
    "    temporal_featuresets.append(features.Featureset(df=temporal_feats,\n",
    "                                                    name=epoch,\n",
    "                                                    id_col=dataset.id_col)\n",
    "                               )\n",
    "temporal_featuresets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation sanity check\n",
    "for fs in temporal_featuresets:\n",
    "    print(fs.df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(10,10)})\n",
    "sns.heatmap(temporal_featuresets[0].df.corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fs in temporal_featuresets:\n",
    "    fs.df.drop(columns=['event_time_min', 'event_time_max'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine static and dynamic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_static_featuresets = []\n",
    "\n",
    "''' For now, let's hand-select\n",
    "Later we will run all possible combos...if it doesn't take for-freakin ever'''\n",
    "\n",
    "for subset in ['FACTB Subset', 'FACTG Subset', 'FACTG Subscales Subset']:\n",
    "    selected_static_featuresets.append(\n",
    "        next(\n",
    "            (fs for fs in static_featuresets \n",
    "             if fs.name == 'demographics + study_behavior + medical + scores + other - ' + subset), \n",
    "            None\n",
    "        )\n",
    "    )\n",
    "n_static_featuresets = len(selected_static_featuresets)\n",
    "print(n_static_featuresets)\n",
    "selected_static_featuresets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "target_col = 'adherent'\n",
    "\n",
    "# Test the performance for a range of lags (number of previous inputs)\n",
    "'''TODO - ask Laura and Mehdi about best way to tune number of lags \n",
    "(using temporal data only? Temporal and a randomly-selected subset of static data?)\n",
    "'''\n",
    "for temporal_feats in temporal_featuresets:\n",
    "    \n",
    "    # Convert adherence_rate into a binary indicator of adherence\n",
    "    temporal_feats.df[target_col] = temporal_feats.df['adherence_rate'].apply(\n",
    "        lambda x: 1 if x > consts.ADHERENCE_THRESHOLD else 0\n",
    "    )\n",
    "    # Drop the original column\n",
    "    temporal_feats.df.drop(columns=['adherence_rate'], inplace=True)\n",
    "\n",
    "    # Set the target col\n",
    "    temporal_feats.target_col = target_col\n",
    "    \n",
    "    for n_lags in range(2, 16):\n",
    "        results = []\n",
    "        for i in range(-1, n_static_featuresets):\n",
    "\n",
    "            # Get a lagged featureset\n",
    "            all_feats = temporal_feats.get_lagged_featureset(epoch=epoch, n_lags=n_lags)\n",
    "        \n",
    "            # For the first index (i== -1), get results for only the temporal feats on their own\n",
    "            if i >= 0:\n",
    "                            \n",
    "                # Otherwise, create a combined set with one of the static featuresets\n",
    "                all_feats = all_feats.create_combined_featureset(fs=selected_static_featuresets[i])\n",
    "\n",
    "            '''Perform final encoding, scaling, etc\n",
    "            Be sure we pass in all known categoricals (they're in numeric form - just need to be one-hot encoded)'''\n",
    "            all_feats.prep_for_modeling()\n",
    "\n",
    "            # Do our actual predictions\n",
    "            res = models.predict(all_feats, n_lags)\n",
    "            results.append(res)\n",
    "        \n",
    "        lag_results = pd.concat(results, axis=0)\n",
    "        lag_results.to_csv('results/prediction_results_' + str(n_lags) + '_lags.csv',index=False)\n",
    "        all_results.append(lag_results)\n",
    "\n",
    "results = pd.concat(all_results, axis=0)\n",
    "results.to_csv('results/prediction_results_all.csv',index=False)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcpn_mems",
   "language": "python",
   "name": "bcpn_mems"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
