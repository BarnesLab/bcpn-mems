{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import math\n",
    "from datetime import datetime\n",
    "import re\n",
    "import csv\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.stats.api as sms\n",
    "import scipy\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import tensorflow as tf\n",
    "from pipeline import data, features, consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "datafile = Path(\"/mnt/c/Users/anbag/Box Sync/Research/UVA/Medication Adherance/MEMS dataset/final_merged_set_v6.csv\")\n",
    "#datafile = Path(\"/mnt/c/Users/ab5bt/Box Sync/Research/UVA/Medication Adherance/MEMS dataset/final_merged_set_v6.csv\")\n",
    "df = pd.read_csv(datafile, parse_dates=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "Thank you to Jason Brownlee\n",
    "https://machinelearningmastery.com/basic-data-cleaning-for-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Dataset class\n",
    "dataset = data.Dataset(df, id_cols = ['PtID', 'MemsNum'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidy the dataset - get rid of useless or recoded columns\n",
    "dataset.clean(to_rename = {**consts.RENAMINGS['demographics'], \n",
    "                             **consts.RENAMINGS['medical']}, \n",
    "              to_drop=[col for col in dataset.df.columns if '_Name' in col],\n",
    "              to_map = consts.CODEBOOK,\n",
    "              to_binarize = ['race_other']\n",
    "             )\n",
    "\n",
    "''' \n",
    "Set dtypes on remaining columns\n",
    "For now, naively assume we only have numerics, datetimes, or objects\n",
    "'''\n",
    "dtypes_dict = {\n",
    "    'numeric': [col for col in dataset.df.columns if 'date' not in col.lower()],\n",
    "    'datetime': ['DateEnroll']\n",
    "}\n",
    "\n",
    "dataset.set_dtypes(dtypes_dict)\n",
    "\n",
    "''' See if it worked '''\n",
    "dataset.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into inputs and outputs\n",
    "X = dataset.df.drop(columns=['percentMEMS8'] + dataset.id_cols, axis=1)\n",
    "X = X.select_dtypes('number').fillna(-1).values\n",
    "y = dataset.df['percentMEMS8'].values\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# define thresholds to check\n",
    "thresholds = np.arange(0.0, 0.55, 0.05)\n",
    "\n",
    "# apply transform with each threshold\n",
    "results = list()\n",
    "\n",
    "for t in thresholds:\n",
    "    # define the transform\n",
    "    transform = VarianceThreshold(threshold=t)\n",
    "    \n",
    "    # transform the input data\n",
    "    X_sel = transform.fit_transform(X)\n",
    "    \n",
    "    # determine the number of input features\n",
    "    n_features = X_sel.shape[1]\n",
    "    print('>Threshold=%.2f, Numeric Features=%d' % (t, n_features))\n",
    "    \n",
    "    # store the result\n",
    "    results.append(n_features)\n",
    "\n",
    "#plot the threshold vs the number of selected features\n",
    "plt.plot(thresholds, results)\n",
    "plt.show()\n",
    "\n",
    "# Note to self - may want to set a high threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Handle the special case of race, which was passed in one-hot-encoded\n",
    "Let's convert it back so we can do imputation a bit later\n",
    "Make sure to do this before we define our feature columns!\n",
    "'''\n",
    "race_cols = [col for col in dataset.df.columns if 'race' in col]\n",
    "dataset.df['race'] = dataset.df[race_cols].idxmax(1)\n",
    "dataset.df.drop(race_cols, inplace=True)\n",
    "dataset.df['race'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.df['race']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate static (non-temporal) features from measures such as validate instruments (e.g., FACTB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    " Organize the candidate features into useful categories for later reference\n",
    " A bit tedious, but helpful \n",
    "'''\n",
    "\n",
    "# Set our excluded features, before anything else\n",
    "excluded = ['percentMEMS8']  # Overall adherence rate - unlikely to be used since we're building weekly vectors\n",
    "\n",
    "feat_cols = {\n",
    "    'demographics': [v for v in consts.RENAMINGS['demographics'].values()\n",
    "                     if v in dataset.df.columns] + ['race'], #add the new, single race col\n",
    "    'study_behavior': [col for col in ['DateEnroll', 'Group', 'complete_4', \n",
    "                                       'complete_8', 'memsuse', 'deceased',\n",
    "                                       'day_miss_fromB', 'day_miss_from7', 'total_days_8'] \n",
    "                       if col in dataset.df.columns],\n",
    "    'medical': [v for v in consts.RENAMINGS['medical'].values() if v in dataset.df.columns] + \\\n",
    "               [col for col in ['stage', 'early_late', 'diagtoenroll'] \n",
    "                if col in dataset.df.columns]\n",
    "}\n",
    "\n",
    "''' This dataset has several repeated measures for validated instruments, \n",
    "such as the FACTB\n",
    "\n",
    "Columns for repeated measures for the same instrument share a suffix (e.g., '_FACTB')\n",
    "Use regex to populate the `scores` category subdictionary quickly, using these suffixes\n",
    "''' \n",
    "\n",
    "for k,v in consts.SCORES.items():\n",
    "    ''' Handle special case of BCPT before doing anything else '''\n",
    "    if k == 'BCPT':\n",
    "        dataset.df.drop(\n",
    "            list(dataset.df.filter(regex = '_BCPT\\d*YN$')), \n",
    "            axis = 1, \n",
    "            inplace = True\n",
    "        )\n",
    "        dataset.df.drop(\n",
    "            list(dataset.df.filter(regex = '_BCPT\\d*O$')), \n",
    "            axis = 1, \n",
    "            inplace = True\n",
    "        )\n",
    "    \n",
    "    '''Some measures weren't precalculated in full for timepoints A, B, and C\n",
    "    Let's fix this ''' \n",
    "    if v['precalculated'] == False:\n",
    "        \n",
    "        ''' For each timepoint, get the aggregate score and add it to the dataset\n",
    "        as a new column'''\n",
    "        for prefix in ['A', 'B', 'C']:\n",
    "            score_cols = list(\n",
    "                dataset.df.filter(regex='^' + prefix + v['suffix'] + '\\d*').columns\n",
    "            )\n",
    "            dataset.df[prefix + v['suffix']] = dataset.df[score_cols].sum(axis=1)\n",
    "\n",
    "    ''' Now that we've calculated everything, get the aggregate score for each measure, \n",
    "    at each timepoint'''\n",
    "    feat_cols['scores_' + k] = list(\n",
    "        dataset.df.filter(regex='^[A-C]' + v['suffix'] + '$').columns\n",
    "    )\n",
    "    \n",
    "''' Create a catch-all category of remaining features, to ensure we got everything '''\n",
    "feat_cols['other'] = [col for col in dataset.df.columns \n",
    "                      if col not in list(itertools.chain(*feat_cols.values())) # exclude anything already in the list\n",
    "                      and not any(prefix in col for prefix in ['A_', 'B_', 'C_']) # exclude individual score cols\n",
    "                      and 'date' not in col \n",
    "                      and col not in dataset.id_cols\n",
    "                      and col not in excluded\n",
    "                     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create new columns for several demographic and medical variables\n",
    "Be sure we update the feature columns dictionary '''\n",
    "demog_drug_cols = [col for col in dataset.df.columns if 'A_DEMO13DRUG' in col]\n",
    "newcol = 'DEMOG_numdrugs'\n",
    "dataset.df[newcol] = dataset.df[demog_drug_cols].count(axis=1)\n",
    "feat_cols['demographics'] = feat_cols['demographics'] + [newcol]\n",
    "\n",
    "post_exam_cols = [col for col in dataset.df.columns if 'C_MR5_date' in col]\n",
    "dataset.df[post_exam_cols] = dataset.df[post_exam_cols].apply(\n",
    "    lambda x: pd.to_datetime(x, errors='coerce')\n",
    ")\n",
    "newcol = 'C_numexams'\n",
    "dataset.df[newcol] = dataset.df[post_exam_cols].count(axis=1)\n",
    "feat_cols['medical'] = feat_cols['medical'] + [newcol]\n",
    "\n",
    "''' Thank you @benvc!\n",
    "https://stackoverflow.com/questions/54367491/calculate-average-of-days-between-a-list-of-dates\n",
    "'''\n",
    "\n",
    "newcol = 'mean_days_betw_exams'\n",
    "dataset.df[newcol] = dataset.df[post_exam_cols].apply(\n",
    "    lambda x: features.mean_days_between_dates(x),\n",
    "    axis=1\n",
    ")\n",
    "feat_cols['medical'] = feat_cols['medical'] + [newcol]\n",
    "\n",
    "\n",
    "# TODO: Dates aren't necessarily in order. Ask Kristi if this is a data entry issue or \n",
    "# An ordering issue?\n",
    "\n",
    "print(dataset.df['DEMOG_numdrugs'].head())\n",
    "print(dataset.df['C_numexams'].head())\n",
    "print(dataset.df['mean_days_betw_exams'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic (Temporal Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract temporal features by converting main dataset's df from wide-form to long-form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "# Get a list of all date columns\n",
    "date_cols = list(dataset.df.filter(regex='date\\d{3}$').columns)\n",
    "\n",
    "i = 0\n",
    "for col in date_cols:\n",
    "\n",
    "    ''' Find all the time cols for that date col'''\n",
    "    time_cols = list(dataset.df.filter(\n",
    "        regex='MEMS_{date_col}_time\\d{{1}}$'.format(date_col=col)).columns)  \n",
    "\n",
    "    '''\n",
    "        Perform a melt so we get MEMS events stratified by patient\n",
    "        Be sure to include the \"within range\" column as one of the id_vars\n",
    "    ''' \n",
    "    additional_cols = [\n",
    "        {\n",
    "            'original': 'MEMS_' + col + '_numtimes',\n",
    "            'new': 'num_times_used_today'\n",
    "        }\n",
    "    ]\n",
    "    if i > 0: # The first date won't have an interval or withinrange\n",
    "        additional_cols.append(\n",
    "            {\n",
    "                'original': 'MEMS_' + col + '_interval',\n",
    "                'new': 'interval'\n",
    "            }\n",
    "        )\n",
    "        additional_cols.append(\n",
    "            {\n",
    "                'original': 'MEMS_' + col + '_withinrange',\n",
    "                'new': 'adherent_today'\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    all_id_cols = dataset.id_cols + ['DateEnroll', col] + [x['original'] for x in additional_cols]\n",
    "    \n",
    "    res = dataset.df[all_id_cols + time_cols].melt(id_vars = all_id_cols)\n",
    "    \n",
    "    ''' Tidy up the resulting dataframe '''\n",
    "    res.rename(columns={col: 'date', 'value': 'time', 'variable': 'MEMS_day'}, \n",
    "               inplace=True)\n",
    "\n",
    "    res['MEMS_day'] =  res['MEMS_day'].apply(lambda x: int(re.sub(r'_time\\d*$', '', x.split('MEMS_date')[1])))\n",
    "    \n",
    "    res.rename(columns={x['original']:x['new'] for x in additional_cols},\n",
    "               inplace=True)\n",
    "\n",
    "#     res.drop(columns=['variable'], inplace=True)\n",
    "    \n",
    "    ''' Finally, merge results into the new dataframe ''' \n",
    "    if df.empty:\n",
    "        df = res.copy()\n",
    "    else:\n",
    "        df = df.append(res, ignore_index=True)\n",
    "    i += 1\n",
    "\n",
    "# Create combined datetime column\n",
    "df['datetime'] = df.apply(\n",
    "    lambda x: features.get_datetime_col(x), axis=1\n",
    ")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "\n",
    "# Fix dtypes\n",
    "df[['adherent_today', 'num_times_used_today']] = df[['adherent_today', 'num_times_used_today']].fillna(0).astype(int)\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "df['interval'] = pd.to_timedelta(df['interval'])\n",
    "\n",
    "\n",
    "# Add binary indicator of any usage (not just number of times used) on a given day\n",
    "df['used_today'] = df['num_times_used_today'].apply(\n",
    "    lambda x: 1 if x > 0 else 0\n",
    ")\n",
    "\n",
    "# Drop rows with an empty date column\n",
    "df.dropna(subset=['date'], inplace=True)\n",
    "\n",
    "'''Remove observations that occurred before a subject's enrollment date\n",
    "Don't remove empty observations just yet\n",
    "TODO: verify original  adherence rates are correct''' \n",
    "df = df.loc[df['DateEnroll'] < df['date']]\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate epochs of interest (time of day, weekday, day/month of study, etc)\n",
    "time_of_day_props = {\n",
    "    'bins': [-1, 6, 12, 18, 24],\n",
    "    'labels': ['late_night','morning', 'afternoon', 'evening']\n",
    "}\n",
    "df = features.get_epochs(df, 'DateEnroll', 'PtID',\n",
    "                         time_of_day_props['bins'], \n",
    "                         time_of_day_props['labels'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict to 210 days, per Kristi's documentation\n",
    "df = df[df['MEMS_day'] <= 210] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that we calculated days of adherence correctly\n",
    "df2 = df.groupby(dataset.id_cols + ['MEMS_day'])['adherent_today'].max().reset_index()\n",
    "df2 = df2.groupby(dataset.id_cols)['adherent_today'].sum().reset_index()\n",
    "df2 = df2.merge(dataset.df[dataset.id_cols + ['total_days_8']])\n",
    "df2.head(30)\n",
    "# Check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Start our final temporal features dataframe -----------\n",
    "\n",
    "# Exclude first month (ramp-up period during which time users were getting used to the MEMS caps)\n",
    "df = df[df['study_month'] > 0]\n",
    "\n",
    "\n",
    "'''Group by our desired epoch and add standard metrics such as mean, std for a given datetime column \n",
    "\n",
    "We want to predict weekly adherence, we we'll group by the study week\n",
    "\n",
    "We could also swap this out for day or month\n",
    "'''\n",
    "groupby_cols = dataset.id_cols + ['study_week']\n",
    "\n",
    "temporal_feats = features.calc_standard_temporal_metrics(df, groupby_cols, 'datetime')\n",
    "\n",
    "''' Calculate adherence-related metrics \n",
    "    \n",
    "    Recall that participants can have multiple observations per day, and that binary indicator columns\n",
    "      ending in '_today' (e.g., 'adherent today') will be the same for each observation\n",
    "      \n",
    "    So we should do an extra step here to ensure we aren't counting metrics multiple times per day...\n",
    "'''\n",
    "df2 = df.groupby(groupby_cols + ['study_day'])['used_today'].max().reset_index() # Max will be 1 or 0\n",
    "df2 = df2.groupby(groupby_cols)['used_today'].agg({\n",
    "    'usage_rate': lambda x: x.sum() / consts.DAYS_IN_WEEK\n",
    "}).reset_index()\n",
    "temporal_feats = temporal_feats.merge(df2, on=groupby_cols)\n",
    "\n",
    "df2 = df.groupby(groupby_cols + ['study_day'])['adherent_today'].max().reset_index() # Max will be 1 or 0\n",
    "df2 = df2.groupby(groupby_cols)['adherent_today'].agg({\n",
    "    'adherence_rate': lambda x: x.sum() / consts.DAYS_IN_WEEK\n",
    "}).reset_index()\n",
    "temporal_feats = temporal_feats.merge(df2, on=groupby_cols)\n",
    "\n",
    "temporal_feats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_to_extract = feat_cols.copy()\n",
    "feats_to_extract.pop('other')\n",
    "static_feats = dataset.build_df_from_features(feats_to_extract)\n",
    "\n",
    "all_feats = temporal_feats.merge(static_feats, on=dataset.id_cols)\n",
    "all_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation & One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode imputation\n",
    "cat_cols = list(consts.CODEBOOK.keys())\n",
    "for col in cat_cols:\n",
    "    all_feats[col].fillna(all_feats[col].mode()[0], inplace=True)\n",
    "\n",
    "all_feats = pd.get_dummies(all_feats, columns=cat_cols) \n",
    "all_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = list(set(list(all_feats.select_dtypes('number').columns)) -\\\n",
    "                    set(dataset.id_cols + ['study_week']) -\\\n",
    "                    set(cat_cols) # Exclude categoricals\n",
    "                   )\n",
    "imputer = IterativeImputer(random_state=5)\n",
    "all_feats[numeric_cols] = imputer.fit_transform(all_feats[numeric_cols])\n",
    "all_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feats[['race_other']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "all_feats[numeric_cols] = scaler.fit_transform(all_feats[numeric_cols])\n",
    "all_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO - think about other features that are important (e.g., does month of enrollment matter?)\n",
    "Add sliding window code and test\n",
    "Figure out how to normalize heterogeneous vals\n",
    "Ask Kristi how they got the days of use and percentage vals...\n",
    "Figure out how to add in cross-validation and tuning, and upsampling in tensorflow\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feats.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Taken from official Tensorflow tutorial on time series data\n",
    "https://www.tensorflow.org/tutorials/structured_data/time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate labels\n",
    "all_feats['adherent'] = all_feats['adherence_rate'].apply(\n",
    "    lambda x: 1 if x > consts.ADHERENCE_THRESHOLD else 0\n",
    ")\n",
    "\n",
    "all_feats.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_indices = {name: i for i, name in enumerate(all_feats.columns)}\n",
    "\n",
    "n = len(all_feats)\n",
    "train_df = all_feats[0:int(n*0.7)]\n",
    "val_df = all_feats[int(n*0.7):int(n*0.9)]\n",
    "test_df = all_feats[int(n*0.9):]\n",
    "\n",
    "num_features = df.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = features.WindowGenerator(input_width=24, label_width=1, shift=1,\n",
    "                              train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "                              label_columns=['adherent'])\n",
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack three slices, the length of the total window:\n",
    "example_window = tf.stack([np.array(train_df[:w1.total_window_size]),\n",
    "                           np.array(train_df[100:100+w1.total_window_size]),\n",
    "                           np.array(train_df[200:200+w1.total_window_size])])\n",
    "\n",
    "\n",
    "example_inputs, example_labels = w2.split_window(example_window)\n",
    "\n",
    "print('All shapes are: (batch, time, features)')\n",
    "print(f'Window shape: {example_window.shape}')\n",
    "print(f'Inputs shape: {example_inputs.shape}')\n",
    "print(f'labels shape: {example_labels.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "#  Categorize each participant according to their score(s) and \n",
    "#  add category to each observation\n",
    "# '''\n",
    "# score_props = {k + '_mean': {'bins': [-1, v['max_val'] / 3.0, \n",
    "#                                       2 * v['max_val'] / 3.0, \n",
    "#                                       v['max_val']],\n",
    "#                              'labels': ['low', 'medium', 'high']\n",
    "#                             }\n",
    "#                             for k, v in consts.SCORES.items()}\n",
    "\n",
    "# dataset = features.gen_categories(dataset, score_props)\n",
    "# dataset.rename({k + '_mean_cat': k + '_cat' for k in consts.SCORES.keys()},\n",
    "#                 axis=1,\n",
    "#                 inplace=True\n",
    "#               )\n",
    "# for k in consts.SCORES.keys():\n",
    "#     target_col = k + '_cat'\n",
    "#     print(target_col)\n",
    "#     id_cols = ['PtID', 'MemsNum']\n",
    "#     mems_df = mems_df.merge(dataset[id_cols + [target_col]], on=id_cols)\n",
    "\n",
    "# mems_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes no sense - fix\n",
    "import seaborn as sns\n",
    "sns.lineplot(x='study_month', y='adherent', hue='FACTB_cat', data=mems_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize adherence frequency\n",
    "df = mems_df['PtID'].value_counts().reset_index(name='count')\n",
    "df.rename(columns={'index': 'PtID'}, inplace=True)\n",
    "df\n",
    "\n",
    "dataset.hist(column='percentMEMS8')\n",
    "\n",
    "# Skewed right - most people took their meds :O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mems_df[['PtID', 'MemsNum'] + feat_cols['demographics'] + \n",
    "        feat_cols['study_behavior'] + \n",
    "        list(itertools.chain(*[v for k,v in feat_cols['scores'].items()]))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['percentMEMS8']*100 > 85] # TODO: Check the lit for preferred threshold for adherence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible research questions\n",
    "- Can MEMS data (frequency, times of day, etc) be used to predict different types of wellbeing? (e.g., social wellbeing)\n",
    "- Can demographic + wellbeing data at baseline (does it have to just be at baseline?) be used to predict long-term adherence?\n",
    "\n",
    "- Worth looking at both of these?\n",
    "\n",
    "- What are the most important determinants of adherence? wellbeing? demographics?\n",
    "- What kind of phenotypes emerge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average scores for assessments that were administered at multiple timepoints\n",
    "for k,v in scores.items():\n",
    "    newcol = k + '_mean'\n",
    "    \n",
    "    # Filtering is necessary here since the FACT-B and FACT-G item-by-item scores are included in the dataset\n",
    "    data[newcol] = data[feat_cols['scores'][k]].mean(axis=1)\n",
    "    \n",
    "    # Be sure to include this new \"mean\" column in our list of feature columns for this score\n",
    "    feat_cols['scores'][k].append(newcol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores_df = data[[col for col in data.columns if 'mean' in col]]\n",
    "\n",
    "for col in mean_scores_df.columns:\n",
    "    fig, ax = plt.subplots()\n",
    "    s = mean_scores_df[col]\n",
    "    print(scipy.stats.describe(s))\n",
    "    print('25th Percentile: ' + str(np.percentile(s, 25)))\n",
    "    print('75th Percentile: ' + str(np.percentile(s, 75)))\n",
    "    print('median: ' + str(s.median()))\n",
    "    sns.distplot(s)\n",
    "    plt.show()\n",
    "    \n",
    "# Most scores skewed toward higher quality of life. Use Median when dividing them up into 'high/low'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Ask Kristi about any stat results they've already completed\n",
    "df = mems_df['PtID'].value_counts().reset_index(name='n_mems_events')\n",
    "df.rename(columns={'index': 'PtID'}, inplace=True)\n",
    "\n",
    "df2 = data[['PtID'] + [col for col in data.columns if 'mean' in col]]\n",
    "for col in df2.columns[1:]:\n",
    "    df[col + '_group'] = df2[col].apply(lambda x: 'low' if x < df2[col].median() else 'high')\n",
    "\n",
    "df = df.merge(df2, on='PtID')\n",
    "\n",
    "group_cols = [col for col in df.columns if 'group' in col]\n",
    "\n",
    "for col in group_cols:\n",
    "    df3 = df.groupby([col])['n_mems_events'].mean().reset_index(name='avg_n_mems_events')\n",
    "    print(df[col].value_counts())\n",
    "    sns.barplot(y='avg_n_mems_events', x=col, data=df3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a demographics dataframe with meaningful labels\n",
    "dem_df = data[feat_cols['demographics']]\n",
    "for col in dem_df.columns:\n",
    "    dem_df[col] = dem_df[col].map(codebook[col])\n",
    "    \n",
    "dem_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in dem_df.columns:\n",
    "\n",
    "    df = dem_df.groupby(\n",
    "        col\n",
    "    ).size().reset_index(name='N').sort_values(by=['N'], ascending=False)\n",
    "    df\n",
    "\n",
    "    pie, ax = plt.subplots(figsize=[10,6])\n",
    "    labels = df[col]\n",
    "    plt.pie(x=df['N'], autopct=\"%.1f%%\", labels=labels, pctdistance=0.5)\n",
    "    plt.title(col.capitalize(), fontsize=14);\n",
    "    # pie.savefig('results/figures/demographics_edu.png', bbox_inches=\"tight\")\n",
    "\n",
    "    df['percentage'] = round(100 * df['N'] / df['N'].sum(), 0)\n",
    "    # df.to_csv('results/tables/demographics_edu.csv')\n",
    "    df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcpn_mems",
   "language": "python",
   "name": "bcpn_mems"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
