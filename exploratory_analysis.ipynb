{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "import csv\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.stats.api as sms\n",
    "import scipy\n",
    "import datetime\n",
    "from pipeline import data, features, consts\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "datafile = Path(\"/mnt/c/Users/anbag/Box Sync/Research/UVA/Medication Adherance/MEMS dataset/final_merged_set_v6.csv\")\n",
    "df = pd.read_csv(datafile, parse_dates=False)\n",
    "df.head()\n",
    "# Testing pre-commit hook for nbstripout\n",
    "# Another notebook output strip test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Dataset class\n",
    "dataset = data.Dataset(df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify columns to rename - want this project to be human-readable!\n",
    "# TODO: Finish renaming from followup medical record extraction form\n",
    "# Then, normalization, etc\n",
    "\n",
    "'''\n",
    "Note - I manually created any needed dummy vars in SPSS and named them what I wanted\n",
    "Any dummy vars included in this renaming dictionary are vars leftover from Kristi's analysis that\n",
    "weren't renamed in place \n",
    "''' \n",
    "rename_dict = {\n",
    "    'demographics': {\n",
    "        'EDU_RECODE':'education', \n",
    "        'Country': 'country',\n",
    "        'A_DEMO1': 'age',\n",
    "        'A_DEMO31': 'race_white',\n",
    "        'A_DEMO32': 'race_black',\n",
    "        'A_DEMO33': 'race_asian',\n",
    "        'A_DEMO34': 'race_native_hawaiian_pacific_islander',\n",
    "        'A_DEMO35': 'race_native_american',\n",
    "        'A_DEMO36': 'race_other',\n",
    "        'A_DEMO37': 'race_prefer_not_to_answer',\n",
    "        'A_DEMO38': 'race_white',\n",
    "        'A_DEMO2': 'hispanic_latina',\n",
    "        'A_DEMO10': 'years_in_usa',\n",
    "        'A_DEMO11': 'primary_lang',\n",
    "        'A_DEMO13YN': 'take_meds_regularly',\n",
    "    },\n",
    "    'medical': { # From medical record abstraction forms (pre/post)\n",
    "        'stage_recoded': 'stage',\n",
    "        'A_MR1': 'pre_insurance_status',\n",
    "        'A_MR3': 'pre_dx_date',\n",
    "        \n",
    "        # --- Treatments received prior to enrollment --- \n",
    "        'A_MR4_YN': 'pre_radiation',\n",
    "        'A_MR5_YN': 'pre_chemo',\n",
    "        'A_MR6_YN': 'pre_surgery',\n",
    "        'A_MR7_YN': 'pre_reconstructive_surgery',\n",
    "         #  --- Drugs prescribed prior to enrollment --- \n",
    "        'A_MR9_tamox_YN': 'pre_tamoxifen',\n",
    "        'A_MR9_ralox_YN': 'pre_raloxifene',\n",
    "        'A_MR9_torem_YN': 'pre_toremifene',\n",
    "        'A_MR9_anas_YN': 'pre_anastrazole',\n",
    "        'A_MR9_exem_YN': 'pre_exemestane',\n",
    "        'A_MR9_let_YN': 'pre_letrozole',\n",
    "        'A_MR9_gose_YN': 'pre_goserelin',\n",
    "        #  --- Comorbid conditions prior to enrollment --- \n",
    "        'A_MR10_a': 'pre_myocardial_infarction', \n",
    "        'A_MR10_b': 'pre_cong_heart_failure',\n",
    "        'A_MR10_c': 'pre_peripheral_vascular_disease',\n",
    "        'A_MR10_d': 'pre_cerebrovascular_disease',\n",
    "        'A_MR10_e': 'pre_dementia',\n",
    "        'A_MR10_f': 'pre_chromic_pulm_disease',\n",
    "        'A_MR10_g': 'pre_conn_tissue_disease',\n",
    "        'A_MR10_h': 'pre_peptic_ulcer_disease',\n",
    "        'A_MR10_i': 'pre_liver_disease_mild',\n",
    "        'A_MR10_j': 'pre_diabetes',\n",
    "        'A_MR10_k': 'pre_hemiplegia',\n",
    "        'A_MR10_l': 'pre_renal_disease',\n",
    "        'A_MR10_m': 'pre_diabetes_and_organdamage',\n",
    "        'A_MR10_n': 'pre_leukemia',\n",
    "        'A_MR10_o': 'pre_lymphoma',\n",
    "        'A_MR10_p': 'pre_liver_disease_mod_severe',\n",
    "        'A_MR10_q': 'pre_met_tumor',\n",
    "        'A_MR10_r': 'pre_aids',\n",
    "        'A_MR10_s': 'pre_cancer_other',\n",
    "        'A_MR10_t': 'pre_arthritis',\n",
    "        'A_MR10_u': 'pre_mental_illness',\n",
    "        # ---- Procedures recommended / received since enrollment --- \n",
    "        'C_MR3': 'post_num_exams_recommended',\n",
    "        'C_MR4': 'at_least_one_exam',\n",
    "        'C_MR5': 'post_all_exams_obtained',\n",
    "        'C_MR6': 'post_num_mammograms_recommended',\n",
    "        'C_MR7_RECODED': 'post_any_mammograms_received',\n",
    "        'C_MR7_YN_RECODED': 'post_all_mammograms_received',\n",
    "        # ---- Drugs prescribed since enrollment ---\n",
    "        'C_MR9_tamox_YN': 'post_tamoxifen',\n",
    "        # Note that `evist` and `ralox` prefixes both refer to same med\n",
    "        'A_MR9_evist_YN': 'post_raloxifene', \n",
    "        # Same note as above; different prefixes used here, but refer to same respective meds\n",
    "        'A_MR9_fares_YN': 'post_toremifene',\n",
    "        'A_MR9_arim_YN': 'post_anastrazole',\n",
    "        'A_MR9_aroma_YN': 'post_exemestane',\n",
    "        'A_MR9_femara_YN': 'post_letrozole',\n",
    "        'A_MR9_zola_YN': 'post_goserelin', \n",
    "        'C_MR10': 'post_changed_meds',\n",
    "        'C_MR12': 'post_side_effects',\n",
    "        'C_MR18': 'post_num_appts_scheduled',\n",
    "        'C_MR19': 'post_num_appts_canceled_by_patient',\n",
    "        'C_MR20': 'post_num_appts_missed',\n",
    "        # C_MR21 through C_MR23 definitely have low variance - not going to bother to rename / include\n",
    "        'C_MR24_MRI': 'post_received_mri',\n",
    "        'C_MR24_US': 'post_received_ultrasound',\n",
    "        'C_MR24_BI': 'post_received_biopsy',\n",
    "        'C_MR24_CT': 'post_received_ct_scan',\n",
    "        'C_MR24_BS': 'post_received_bone_scan',\n",
    "        'C_MR24_AS': 'post_received_addnl_surgery',\n",
    "        'C_MR24_AR': 'post_received_addnl_radiation',\n",
    "        'C_MR24_GT': 'post_received_genetic_couns_test',\n",
    "        'C_MR24_GE': 'post_received_gyn_exam',\n",
    "        'C_MR24_Otherbreastsurgery': 'post_received_other_breast_surgery',\n",
    "        # C_MR25 through C_MR27 definitely have low variance - not going to bother to rename / include\n",
    "    }\n",
    "}\n",
    "# Specify columns to drop\n",
    "drop_dict = [col for col in dataset.df.columns if '_Name' in col]\n",
    "\n",
    "# Tidy the dataset\n",
    "dataset.clean({**rename_dict['demographics'], **rename_dict['medical']}, drop_dict)\n",
    "\n",
    "# See if it worked\n",
    "dataset.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate static (non-temporal) features from measures such as validate instruments (e.g., FACTB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - retain only rows (columns?) with 80% of data\n",
    "\n",
    "''' \n",
    " Create a dictionary of candidate features\n",
    " Keys are categories that are useful if we want to reference certain groups of features\n",
    " Values are lists of columns corresponding their feature categories\n",
    " '''\n",
    "\n",
    "feat_cols = {\n",
    "    'demographics': [v for v in rename_dict['demographics'].values()] + \\\n",
    "                    [col for col in dataset.df.columns if 'education_' in col] + \\\n",
    "                    [col for col in dataset.df.columns if 'marital_status' in col] + \\\n",
    "                    [col for col in dataset.df.columns if 'employment' in col] + \\\n",
    "                    [col for col in dataset.df.columns if 'income' in col] + \\\n",
    "                    [col for col in dataset.df.columns if 'birth_country' in col] + \\\n",
    "                    [col for col in dataset.df.columns if 'primary_lang' in col]\n",
    "                    ,\n",
    "    'study_behavior': ['DateEnroll', 'complete_4', 'complete_8', 'memsuse', 'deceased'],\n",
    "    'medical': [v for v in rename_dict['medical'].values()] + \\\n",
    "               [col for col in dataset.df.columns if 'insurance_status' in col] + \\\n",
    "               [col for col in dataset.df.columns if 'followup_elsewhere' in col] + \\\n",
    "               ['stage', 'early_late', 'diagtoenroll']\n",
    "}\n",
    "\n",
    "'''\n",
    "This dataset has several repeated measures for validated instruments, such as the FACTB\n",
    "Columns for repeated measures for the same instrument share a suffix (e.g., '_FACTB')\n",
    "Use regex to populate the `scores` category subdictionary quickly, using these suffixes\n",
    "\n",
    "TODO: fix this - add back in\n",
    "''' \n",
    "\n",
    "# for k,v in consts.SCORES.items():\n",
    "#     feat_cols['scores_' + k] = list(\n",
    "#         dataset.df.filter(regex='^[A-C]' + v['suffix'] + '$').columns\n",
    "#     )\n",
    "\n",
    "'''\n",
    "Specify the feature column dtypes explicitly\n",
    "Prefer this to one-by-one column spec often used with pandas\n",
    "''' \n",
    "dtypes = {\n",
    "    'numeric': list(itertools.chain(*[v for k,v in feat_cols.items() if 'date' not in k])),\n",
    "    'datetime': list(itertools.chain(*[v for k,v in feat_cols.items() if 'date' in k]))\n",
    "}\n",
    "\n",
    "# '''Set the features and dtypes ''' \n",
    "# dataset.update_features(feat_cols, dtypes)\n",
    "\n",
    "dataset.df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns for several demographic and medical variables\n",
    "demog_drug_cols = [col for col in dataset.df.columns if 'A_DEMO13DRUG' in col]\n",
    "dataset.df['DEMOG_numdrugs'] = dataset.df[demog_drug_cols].count(axis=1)\n",
    "\n",
    "post_exam_cols = [col for col in dataset.df.columns if 'C_MR5_date' in col]\n",
    "dataset.df[post_exam_cols] = dataset.df[post_exam_cols].apply(\n",
    "    lambda x: pd.to_datetime(x, errors='coerce')\n",
    ")\n",
    "dataset.df['C_numexams'] = dataset.df[post_exam_cols].count(axis=1)\n",
    "\n",
    "n_examcols = len(post_exam_cols)\n",
    "for i in range(1,n_examcols):\n",
    "    curr_col = post_exam_cols[i]\n",
    "    prev_col = post_exam_cols[i-1]\n",
    "    dataset.df[curr_col + '_days_since_last_exam'] = dataset.df[curr_col] - dataset.df[prev_col]\n",
    "    dataset.df[curr_col + '_days_since_last_exam'] = dataset.df[curr_col + '_days_since_last_exam'].dt.days\n",
    "    print(dataset.df[curr_col + '_days_since_last_exam'])\n",
    "# x.diff().mean().total_seconds() / (60 * 60 * 24) # mean days between exams\n",
    "\n",
    "# TODO: Dates aren't necessarily in order. Ask Kristi if this is a data entry issue or \n",
    "# An ordering issue?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {\n",
    "    'BACS': {\n",
    "        'semantic_label': 'Barriers to Care Scale',\n",
    "        'suffix': '_BACS',\n",
    "        'max_val': -1 # Scores with max_val == -1 are those left to be calculated\n",
    "    },\n",
    "    'BCPT': {\n",
    "        'semantic_label': 'Breast Cancer Prevention Trial Symptom Checklist',\n",
    "        'suffix': '_BCPT',\n",
    "        'max_val': -1\n",
    "    },\n",
    "    'BCSK': {\n",
    "        'semantic_label': 'Breast Cancer Survivorship Knowledge',\n",
    "        'suffix': '_BCSK',\n",
    "        'max_val': -1\n",
    "    },\n",
    "    'BMQ': {\n",
    "        'semantic_label': 'Beliefs About Medicines Questionnaire',\n",
    "        'suffix': '_BMQ',\n",
    "        'max_val': -1\n",
    "    },\n",
    "    'CASE': {\n",
    "        'semantic_label': 'CASE',\n",
    "        'suffix': '_CASE',\n",
    "        'max_val': -1\n",
    "    },\n",
    "    'DECREG': {\n",
    "        'semantic_label': 'Decision Regret Scale',\n",
    "        'suffix': '_DECREG',\n",
    "        'max_val': -1\n",
    "    },\n",
    "    'FACIT_SP': {\n",
    "        'semantic_label': 'FACIT-SP',\n",
    "        'suffix': '_FACITSP',\n",
    "        'max_val': -1\n",
    "    },\n",
    "    'FACT_B': {\n",
    "        'semantic_label': 'FACT-B',\n",
    "        'suffix': '_FACTB',\n",
    "        'max_val': 148\n",
    "    },\n",
    "    'FACT_BC': {\n",
    "        'semantic_label': 'FACT Breast Cancer Subscale',\n",
    "        'suffix': '_BC',\n",
    "        'max_val': 40\n",
    "    },\n",
    "    'FACT_G_PWB': {\n",
    "        'semantic_label': 'FACT-G Physical Well-Being Subscale',\n",
    "        'suffix': '_PWB',\n",
    "        'max_val': 28\n",
    "    },\n",
    "    'FACT_G_SWB': {\n",
    "        'semantic_label': 'FACT-G Social Physical Well-Being Subscale',\n",
    "        'suffix': '_SWB',\n",
    "        'max_val': 28\n",
    "    },\n",
    "    'FACT_G_EWB': {\n",
    "        'semantic_label': 'FACT-G Emotional Physical Well-Being Subscale',\n",
    "        'suffix': '_EWB',\n",
    "        'max_val': 28\n",
    "    },\n",
    "    'FACT_G_FWB': {\n",
    "        'semantic_label': 'FACT-G Functional Physical Well-Being Subscale',\n",
    "        'suffix': '_FWB',\n",
    "        'max_val': 28\n",
    "    },\n",
    "    'FACT_G': {\n",
    "        'semantic_label': 'FACT-G',\n",
    "        'suffix': '_FACTG',\n",
    "        'max_val': 108\n",
    "    },\n",
    "    'MDASI': {\n",
    "        'semantic_label': 'MD Anderson Symptom Inventory',\n",
    "        'suffix': '_MDASI',\n",
    "        'max_val': -1\n",
    "    },\n",
    "    'MASES': {\n",
    "        'semantic_label': 'Medication Adherence Self-Efficacy Scale',\n",
    "        'suffix': '_MASES',\n",
    "        'max_val': -1\n",
    "    },\n",
    "    'MEDAD': {\n",
    "        'semantic_label': 'Medication Adherence Scale',\n",
    "        'suffix': '_MEDAD',\n",
    "        'max_val': -1\n",
    "    },\n",
    "    'PEARL': {  # I think Kristi said don't use this\n",
    "        'semantic_label': 'Pearlin Mastery Scale',\n",
    "        'suffix': '_PEARL',\n",
    "        'max_val': -1\n",
    "    },\n",
    "    'PTGI': {\n",
    "        'semantic_label': 'Posttraumatic Growth Inventory',\n",
    "        'suffix': '_PTGI',\n",
    "        'max_val': -1\n",
    "    },\n",
    "    'PSAT': {\n",
    "        'semantic_label': 'Patient Satisfaction',\n",
    "        'suffix': '_PSAT',\n",
    "        'max_val': -1\n",
    "    },\n",
    "    'PSS': {\n",
    "        'semantic_label': 'Perceived Stress Scale',\n",
    "        'suffix': '_PSS',\n",
    "        'max_val': -1\n",
    "    },\n",
    "    'PSUSP': {\n",
    "        'semantic_label': 'Perceived Susceptibility Scale',\n",
    "        'suffix': '_PSUSP',\n",
    "        'max_val': -1\n",
    "    },\n",
    "    'SS': {  \n",
    "        'semantic_label:' # Krause and Borwaski-Clark Social Support Scale\n",
    "        'suffix': '_SS',\n",
    "        'max_val': -1\n",
    "    },\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create new columns of metrics for the validated instruments (e.g., mean, std)\n",
    "''' \n",
    "for k in consts.SCORES.keys():\n",
    "    score_category = 'scores_' + k\n",
    "    cols = dataset.features[score_category]\n",
    "    dataset.df, newcols = features.calc_standard_static_metrics(df=dataset.df, cols=cols, col_prefix=k)\n",
    "    \n",
    "    # Add new columns to features dictionary\n",
    "    feat_cols[score_category] = feat_cols[score_category] + newcols\n",
    "\n",
    "# Update the features\n",
    "dataset.update_features(feat_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic (Temporal Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract temporal features by converting main dataset's df from wide-form to long-form\n",
    "If data was already in long form, we wouldn't need all these steps. Alas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cols = ['PtID', 'MemsNum', 'DateEnroll']\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Get a list of all date columns\n",
    "date_cols = list(dataset.df.filter(regex='date\\d{3}$').columns)\n",
    "\n",
    "i = 0\n",
    "for col in date_cols:\n",
    "\n",
    "    ''' Find all the time cols for that date col'''\n",
    "    time_cols = list(dataset.df.filter(\n",
    "        regex='MEMS_{date_col}_time\\d{{1}}$'.format(date_col=col)).columns)  \n",
    "\n",
    "    '''\n",
    "        Perform a melt so we get MEMS events stratified by patient\n",
    "        Be sure to include the \"within range\" column as one of the id_vars\n",
    "    ''' \n",
    "    additional_cols = [\n",
    "        {\n",
    "            'original': 'MEMS_' + col + '_withinrange',\n",
    "            'new': 'adherent_today'\n",
    "        },\n",
    "        {\n",
    "            'original': 'MEMS_' + col + '_numtimes',\n",
    "            'new': 'num_times_used_today'\n",
    "        },\n",
    "    ]\n",
    "    if i > 0: # The first date won't have an interval\n",
    "        additional_cols.append({\n",
    "            'original': 'MEMS_' + col + '_interval',\n",
    "            'new': 'interval'\n",
    "        })\n",
    "    \n",
    "    all_id_cols = id_cols + [col] + [x['original'] for x in additional_cols]\n",
    "    \n",
    "    res = dataset.df[all_id_cols + time_cols].melt(id_vars = all_id_cols)\n",
    "\n",
    "    ''' Tidy up the resulting dataframe '''\n",
    "    res.rename(columns={col: 'date', 'value': 'time'}, \n",
    "               inplace=True)\n",
    "    \n",
    "    res.rename(columns={x['original']:x['new'] for x in additional_cols},\n",
    "               inplace=True)\n",
    "\n",
    "    res.drop(columns=['variable'], inplace=True)\n",
    "    \n",
    "    ''' Finally, merge results into the new dataframe ''' \n",
    "    if df.empty:\n",
    "        df = res.copy()\n",
    "    else:\n",
    "        df = df.append(res, ignore_index=True)\n",
    "    i += 1\n",
    "\n",
    "# Create combined datetime column\n",
    "df['datetime'] = df.apply(\n",
    "    lambda x: features.get_datetime_col(x), axis=1\n",
    ")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "\n",
    "# Fix dtypes\n",
    "df[['adherent_today', 'num_times_used_today']] = df[['adherent_today', 'num_times_used_today']].fillna(0).astype(int)\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "df['interval'] = pd.to_timedelta(df['interval'])\n",
    "\n",
    "\n",
    "# Add binary indicator of any usage (not just number of times used) on a given day\n",
    "df['used_today'] = df['num_times_used_today'].apply(\n",
    "    lambda x: 1 if x > 0 else 0\n",
    ")\n",
    "\n",
    "# Drop rows with an empty date column\n",
    "df.dropna(subset=['date'], inplace=True)\n",
    "\n",
    "# Remove observations that occurred before a subject's enrollment date\n",
    "# Don't remove empty observations just yet - need to verify original adherence rates are correct\n",
    "df = df.loc[df['DateEnroll'] < df['date']]\n",
    "\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate epochs of interest (time of day, weekday, day/month of study, etc)\n",
    "time_of_day_props = {\n",
    "    'bins': [-1, 6, 12, 18, 24],\n",
    "    'labels': ['late_night','morning', 'afternoon', 'evening']\n",
    "}\n",
    "df = features.get_epochs(df, 'DateEnroll', 'PtID',\n",
    "                         time_of_day_props['bins'], \n",
    "                         time_of_day_props['labels'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['interval'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Restrict to the study period (210 days)\n",
    "Kristi's group used only 210 days of data - ask them why\n",
    "Epochs are zero-indexed so upper bound is 209\n",
    "\n",
    "'''\n",
    "df = df[df['study_day'] <= 209] \n",
    "\n",
    "''' \n",
    "Start our final temporal features dataframe\n",
    "\n",
    "First, group by our desired epoch and add standard metrics such as mean, std for a given datetime column \n",
    "We want to predict weekly adherence, we we'll group by the study week\n",
    "We could also swap this out for day or month\n",
    "'''\n",
    "groupby_cols = ['PtID', 'MemsNum', 'study_week']\n",
    "\n",
    "# TODO: Move to consts\n",
    "SECONDS_IN_HOUR = 3600.0\n",
    "\n",
    "temporal_feats = features.calc_standard_temporal_metrics(df, groupby_cols, 'datetime')\n",
    "\n",
    "''' \n",
    "Calculate adherence-related metrics\n",
    "'''\n",
    "\n",
    "adherence_feats = df.groupby(groupby_cols)['used_today'].agg({\n",
    "    'usage_rate': lambda x: x.sum() / 7.0\n",
    "}).reset_index()\n",
    "\n",
    "temporal_feats = temporal_feats.merge(adherence_feats, on=groupby_cols)\n",
    "\n",
    "adherence_feats = df.groupby(groupby_cols)['adherent_today'].agg({\n",
    "    'adherence_rate': lambda x: x.sum() / 7.0\n",
    "}).reset_index()\n",
    "temporal_feats = temporal_feats.merge(adherence_feats, on=groupby_cols)\n",
    "\n",
    "temporal_feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract static features (e.g., scores)\n",
    "id_cols=['PtID', 'MemsNum']\n",
    "static_feats = dataset.build_df_from_features(id_cols=id_cols)\n",
    "\n",
    "# Create featureset from both static and dynamic (temporal) features\n",
    "all_feats = static_feats.merge(temporal_feats, on=id_cols)\n",
    "all_feats.head()\n",
    "\n",
    "'''\n",
    "TODO - think about other features that are important (e.g., does month of enrollment matter?)\n",
    "Add sliding window code and test\n",
    "Figure out how to normalize heterogeneous vals\n",
    "Ask Kristi how they got the days of use and percentage vals...\n",
    "Figure out how to add in cross-validation and tuning, and upsampling in tensorflow\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, convert labels to prep for modeling\n",
    "# First, confirm 'percentMEMS8' column is correct (adherence percentage for whole study) \n",
    "# Getting diff numbers for days of use than Kristi's group did...hmm\n",
    "# For now, switch to my own conversion...\n",
    "all_feats['adherent'] = all_feats['adherence_rate'].apply(\n",
    "    lambda x: 1 if x > .80 else 0\n",
    ")\n",
    "\n",
    "all_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Taken from official Tensorflow tutorial on time series data\n",
    "https://www.tensorflow.org/tutorials/structured_data/time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_indices = {name: i for i, name in enumerate(all_feats.columns)}\n",
    "\n",
    "n = len(all_feats)\n",
    "train_df = all_feats[0:int(n*0.7)]\n",
    "val_df = all_feats[int(n*0.7):int(n*0.9)]\n",
    "test_df = all_feats[int(n*0.9):]\n",
    "\n",
    "num_features = df.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Add Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = features.WindowGenerator(input_width=24, label_width=1, shift=1,\n",
    "                              label_columns=['T (degC)'])\n",
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack three slices, the length of the total window:\n",
    "example_window = tf.stack([np.array(train_df[:w1.total_window_size]),\n",
    "                           np.array(train_df[100:100+w1.total_window_size]),\n",
    "                           np.array(train_df[200:200+w1.total_window_size])])\n",
    "\n",
    "\n",
    "example_inputs, example_labels = w2.split_window(example_window)\n",
    "\n",
    "print('All shapes are: (batch, time, features)')\n",
    "print(f'Window shape: {example_window.shape}')\n",
    "print(f'Inputs shape: {example_inputs.shape}')\n",
    "print(f'labels shape: {example_labels.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "#  Categorize each participant according to their score(s) and \n",
    "#  add category to each observation\n",
    "# '''\n",
    "# score_props = {k + '_mean': {'bins': [-1, v['max_val'] / 3.0, \n",
    "#                                       2 * v['max_val'] / 3.0, \n",
    "#                                       v['max_val']],\n",
    "#                              'labels': ['low', 'medium', 'high']\n",
    "#                             }\n",
    "#                             for k, v in consts.SCORES.items()}\n",
    "\n",
    "# dataset = features.gen_categories(dataset, score_props)\n",
    "# dataset.rename({k + '_mean_cat': k + '_cat' for k in consts.SCORES.keys()},\n",
    "#                 axis=1,\n",
    "#                 inplace=True\n",
    "#               )\n",
    "# for k in consts.SCORES.keys():\n",
    "#     target_col = k + '_cat'\n",
    "#     print(target_col)\n",
    "#     id_cols = ['PtID', 'MemsNum']\n",
    "#     mems_df = mems_df.merge(dataset[id_cols + [target_col]], on=id_cols)\n",
    "\n",
    "# mems_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes no sense - fix\n",
    "import seaborn as sns\n",
    "sns.lineplot(x='study_month', y='adherent', hue='FACTB_cat', data=mems_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize adherence frequency\n",
    "df = mems_df['PtID'].value_counts().reset_index(name='count')\n",
    "df.rename(columns={'index': 'PtID'}, inplace=True)\n",
    "df\n",
    "\n",
    "dataset.hist(column='percentMEMS8')\n",
    "\n",
    "# Skewed right - most people took their meds :O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mems_df[['PtID', 'MemsNum'] + feat_cols['demographics'] + \n",
    "        feat_cols['study_behavior'] + \n",
    "        list(itertools.chain(*[v for k,v in feat_cols['scores'].items()]))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['percentMEMS8']*100 > 85] # TODO: Check the lit for preferred threshold for adherence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible research questions\n",
    "- Can MEMS data (frequency, times of day, etc) be used to predict different types of wellbeing? (e.g., social wellbeing)\n",
    "- Can demographic + wellbeing data at baseline (does it have to just be at baseline?) be used to predict long-term adherence?\n",
    "\n",
    "- Worth looking at both of these?\n",
    "\n",
    "- What are the most important determinants of adherence? wellbeing? demographics?\n",
    "- What kind of phenotypes emerge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average scores for assessments that were administered at multiple timepoints\n",
    "for k,v in scores.items():\n",
    "    newcol = k + '_mean'\n",
    "    \n",
    "    # Filtering is necessary here since the FACT-B and FACT-G item-by-item scores are included in the dataset\n",
    "    data[newcol] = data[feat_cols['scores'][k]].mean(axis=1)\n",
    "    \n",
    "    # Be sure to include this new \"mean\" column in our list of feature columns for this score\n",
    "    feat_cols['scores'][k].append(newcol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores_df = data[[col for col in data.columns if 'mean' in col]]\n",
    "\n",
    "for col in mean_scores_df.columns:\n",
    "    fig, ax = plt.subplots()\n",
    "    s = mean_scores_df[col]\n",
    "    print(scipy.stats.describe(s))\n",
    "    print('25th Percentile: ' + str(np.percentile(s, 25)))\n",
    "    print('75th Percentile: ' + str(np.percentile(s, 75)))\n",
    "    print('median: ' + str(s.median()))\n",
    "    sns.distplot(s)\n",
    "    plt.show()\n",
    "    \n",
    "# Most scores skewed toward higher quality of life. Use Median when dividing them up into 'high/low'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Ask Kristi about any stat results they've already completed\n",
    "df = mems_df['PtID'].value_counts().reset_index(name='n_mems_events')\n",
    "df.rename(columns={'index': 'PtID'}, inplace=True)\n",
    "\n",
    "df2 = data[['PtID'] + [col for col in data.columns if 'mean' in col]]\n",
    "for col in df2.columns[1:]:\n",
    "    df[col + '_group'] = df2[col].apply(lambda x: 'low' if x < df2[col].median() else 'high')\n",
    "\n",
    "df = df.merge(df2, on='PtID')\n",
    "\n",
    "group_cols = [col for col in df.columns if 'group' in col]\n",
    "\n",
    "for col in group_cols:\n",
    "    df3 = df.groupby([col])['n_mems_events'].mean().reset_index(name='avg_n_mems_events')\n",
    "    print(df[col].value_counts())\n",
    "    sns.barplot(y='avg_n_mems_events', x=col, data=df3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a demographics dataframe with meaningful labels\n",
    "dem_df = data[feat_cols['demographics']]\n",
    "for col in dem_df.columns:\n",
    "    dem_df[col] = dem_df[col].map(codebook[col])\n",
    "    \n",
    "dem_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in dem_df.columns:\n",
    "\n",
    "    df = dem_df.groupby(\n",
    "        col\n",
    "    ).size().reset_index(name='N').sort_values(by=['N'], ascending=False)\n",
    "    df\n",
    "\n",
    "    pie, ax = plt.subplots(figsize=[10,6])\n",
    "    labels = df[col]\n",
    "    plt.pie(x=df['N'], autopct=\"%.1f%%\", labels=labels, pctdistance=0.5)\n",
    "    plt.title(col.capitalize(), fontsize=14);\n",
    "    # pie.savefig('results/figures/demographics_edu.png', bbox_inches=\"tight\")\n",
    "\n",
    "    df['percentage'] = round(100 * df['N'] / df['N'].sum(), 0)\n",
    "    # df.to_csv('results/tables/demographics_edu.csv')\n",
    "    df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcpn_mems",
   "language": "python",
   "name": "bcpn_mems"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
